import streamlit as st
import yfinance as yf
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import requests # æ–°å¢é€™è¡Œ
from datetime import datetime, timedelta

# ==========================================
# 0. é é¢è¨­å®š
# ==========================================
st.set_page_config(
    page_title="Nasdaq 100 å‹•èƒ½è¼ªå‹•æˆ°æƒ…å®¤",
    page_icon="ğŸš€",
    layout="wide"
)

# å…§å»ºå‚™ç”¨æ¸…å–® (è¬ä¸€çˆ¬èŸ²æ›æ‰æ™‚çš„ä¿éšª)
STATIC_BACKUP = [
    'AAPL', 'ABNB', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AEP', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'ANSS', 'APP',
    'ASML', 'AVGO', 'AXON', 'AZN', 'BIIB', 'BKNG', 'BKR', 'CCEP', 'CDNS', 'CDW', 'CEG', 'CHTR', 'CMCSA',
    'COST', 'CPRT', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CTSH', 'DDOG', 'DLTR', 'DXCM', 'EA', 'EXC', 'FANG',
    'FAST', 'FTNT', 'GEHC', 'GFS', 'GILD', 'GOOG', 'GOOGL', 'HON', 'IDXX', 'ILMN', 'INTC', 'INTU', 'ISRG',
    'KDP', 'KHC', 'KLAC', 'LIN', 'LRCX', 'LULU', 'MAR', 'MCHP', 'MDLZ', 'MELI', 'META', 'MNST', 'MRNA',
    'MRVL', 'MSFT', 'MU', 'NFLX', 'NVDA', 'NXPI', 'ODFL', 'ON', 'ORLY', 'PANW', 'PAYX', 'PCAR', 'PDD',
    'PEP', 'PYPL', 'QCOM', 'REGN', 'ROP', 'ROST', 'SBUX', 'SIRI', 'SNPS', 'TEAM', 'TMUS', 'TSLA', 'TTD',
    'TXN', 'VRSK', 'VRTX', 'WBA', 'WBD', 'WDAY', 'XEL', 'ZS', 'QQQ'
]

# ==========================================
# 1. æ™ºèƒ½æ¸…å–®ç²å–å‡½æ•¸ (è‡ªå‹•æ›´æ–°)
# ==========================================
@st.cache_data(ttl=86400) # è¨­å®šå¿«å–ï¼šæ¯ 24 å°æ™‚ (86400ç§’) æ‰é‡æ–°çˆ¬ä¸€æ¬¡ï¼Œå…¶ä»–æ™‚é–“ç›´æ¥ç”¨
def get_latest_components():
    """
    è‡ªå‹•æŠ“å– Nasdaq 100 æœ€æ–°æˆåˆ†è‚¡
    """
    tickers = []
    try:
        url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        # å½è£æˆ Chrome ç€è¦½å™¨ï¼Œç¹é 403 Forbidden
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        # ç™¼é€è«‹æ±‚
        r = requests.get(url, headers=headers)
        r.raise_for_status() # æª¢æŸ¥æ˜¯å¦é€£ç·šæˆåŠŸ
        
        # è®€å–è¡¨æ ¼
        tables = pd.read_html(r.text)
        
        # å°‹æ‰¾åŒ…å« Ticker çš„è¡¨æ ¼
        target_table = None
        for t in tables:
            if 'Ticker' in t.columns:
                target_table = t
                break
            elif 'Symbol' in t.columns:
                target_table = t
                break
        
        if target_table is not None:
            col = 'Ticker' if 'Ticker' in target_table.columns else 'Symbol'
            tickers = target_table[col].tolist()
            # è™•ç†ç‰¹æ®Šä»£ç¢¼ (å¦‚ BRK.B -> BRK-B)
            tickers = [t.replace('.', '-') for t in tickers]
            # ç¢ºä¿ QQQ åœ¨è£¡é¢
            if 'QQQ' not in tickers: tickers.append('QQQ')
            return tickers
        else:
            raise ValueError("æ‰¾ä¸åˆ°è¡¨æ ¼")

    except Exception as e:
        # å¦‚æœçˆ¬èŸ²å¤±æ•—ï¼Œéœé»˜åˆ‡æ›åˆ°å‚™ç”¨æ¸…å–®ï¼Œä½†å¯ä»¥åœ¨ Log çœ‹åˆ°
        print(f"âš ï¸ è‡ªå‹•æ›´æ–°å¤±æ•—: {e}ï¼Œåˆ‡æ›è‡³éœæ…‹å‚™ç”¨æ¸…å–®ã€‚")
        return STATIC_BACKUP

# ==========================================
# 2. ç²å–æ•¸æ“šä¸»å‡½æ•¸
# ==========================================
@st.cache_data(ttl=3600)
def get_data(lookback_years=3):
    """ä¸‹è¼‰éå» N å¹´çš„æ•¸æ“š"""
    
    # ã€é—œéµã€‘é€™è£¡ä¸å†ç”¨éœæ…‹è®Šæ•¸ï¼Œè€Œæ˜¯å‘¼å«ä¸Šé¢çš„è‡ªå‹•æ›´æ–°å‡½æ•¸
    current_tickers = get_latest_components()
    
    start_date = (datetime.now() - timedelta(days=lookback_years*365)).strftime('%Y-%m-%d')
    
    # é¡¯ç¤ºç›®å‰æŠ“åˆ°äº†å¹¾æ”¯è‚¡ç¥¨ï¼Œè®“ä½¿ç”¨è€…çŸ¥é“
    st.toast(f'å·²è¼‰å…¥ {len(current_tickers)} æ”¯æœ€æ–°æˆåˆ†è‚¡', icon="âœ…")
    
    with st.spinner(f'æ­£åœ¨ä¸‹è¼‰ {len(current_tickers)} æ”¯æˆåˆ†è‚¡æ•¸æ“š...è«‹ç¨å€™ â˜•'):
        data = yf.download(current_tickers, start=start_date, interval="1d", progress=False, group_by='ticker', auto_adjust=True)
    
    df_close = pd.DataFrame()
    for t in current_tickers:
        try:
            if t in data.columns.levels[0]:
                series = data[t]['Close']
                if len(series.dropna()) > 200:
                    df_close[t] = series
        except:
            pass
    
    df_close = df_close.fillna(method='ffill').dropna(how='all')
    df_close.index = pd.to_datetime(df_close.index).tz_localize(None)
    return df_close

# ... (ä»¥ä¸‹ç¨‹å¼ç¢¼ä¿æŒä¸è®Šï¼Œç›´æ¥å¾ def calculate_metrics é–‹å§‹æ¥çºŒ) ...
